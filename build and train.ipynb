{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mexican-frost",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T15:39:01.972595Z",
     "start_time": "2021-02-06T15:38:58.360336Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "graduate-navigator",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T15:39:01.983335Z",
     "start_time": "2021-02-06T15:39:01.974504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1107547 characters\n"
     ]
    }
   ],
   "source": [
    "text = open('ptter4.txt', 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "infrared-serial",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T15:39:01.994689Z",
     "start_time": "2021-02-06T15:39:01.986498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HARRY POTTER AND THE GOBLET OF FIRE\n",
      "\n",
      "CHAPTER ONE - THE RIDDLE HOUSE\n",
      "\n",
      "\tThe villagers of Little Hangleron still called it \"the Riddle House,\" even though it had been many years since the Riddle family had lived there.  It stood on a hill overlooking th\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "better-martial",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T15:39:02.047094Z",
     "start_time": "2021-02-06T15:39:01.997565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "boxed-infrared",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T15:39:02.124893Z",
     "start_time": "2021-02-06T15:39:02.049338Z"
    }
   },
   "outputs": [],
   "source": [
    "ids_from_chars = preprocessing.StringLookup(\n",
    "    vocabulary=list(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "historical-colorado",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T15:39:02.149664Z",
     "start_time": "2021-02-06T15:39:02.131389Z"
    }
   },
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "settled-merchandise",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T15:39:02.158968Z",
     "start_time": "2021-02-06T15:39:02.152728Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sixth-requirement",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T15:39:02.893276Z",
     "start_time": "2021-02-06T15:39:02.166750Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1107547,), dtype=int64, numpy=array([33, 26, 43, ..., 12,  3,  3])>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "lightweight-integral",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T15:39:02.899977Z",
     "start_time": "2021-02-06T15:39:02.895346Z"
    }
   },
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ordinary-municipality",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T15:39:02.946773Z",
     "start_time": "2021-02-06T15:39:02.901632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H\n",
      "A\n",
      "R\n",
      "R\n",
      "Y\n",
      " \n",
      "P\n",
      "O\n",
      "T\n",
      "T\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "taken-wound",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T15:39:02.951864Z",
     "start_time": "2021-02-06T15:39:02.948838Z"
    }
   },
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "refined-cigarette",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T15:39:02.979151Z",
     "start_time": "2021-02-06T15:39:02.960086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'H' b'A' b'R' b'R' b'Y' b' ' b'P' b'O' b'T' b'T' b'E' b'R' b' ' b'A'\n",
      " b'N' b'D' b' ' b'T' b'H' b'E' b' ' b'G' b'O' b'B' b'L' b'E' b'T' b' '\n",
      " b'O' b'F' b' ' b'F' b'I' b'R' b'E' b'\\n' b'\\n' b'C' b'H' b'A' b'P' b'T'\n",
      " b'E' b'R' b' ' b'O' b'N' b'E' b' ' b'-' b' ' b'T' b'H' b'E' b' ' b'R'\n",
      " b'I' b'D' b'D' b'L' b'E' b' ' b'H' b'O' b'U' b'S' b'E' b'\\n' b'\\n' b'\\t'\n",
      " b'T' b'h' b'e' b' ' b'v' b'i' b'l' b'l' b'a' b'g' b'e' b'r' b's' b' '\n",
      " b'o' b'f' b' ' b'L' b'i' b't' b't' b'l' b'e' b' ' b'H' b'a' b'n' b'g'\n",
      " b'l' b'e' b'r'], shape=(101,), dtype=string)\n",
      "tf.Tensor(\n",
      "[b'o' b'n' b' ' b's' b't' b'i' b'l' b'l' b' ' b'c' b'a' b'l' b'l' b'e'\n",
      " b'd' b' ' b'i' b't' b' ' b'\"' b't' b'h' b'e' b' ' b'R' b'i' b'd' b'd'\n",
      " b'l' b'e' b' ' b'H' b'o' b'u' b's' b'e' b',' b'\"' b' ' b'e' b'v' b'e'\n",
      " b'n' b' ' b't' b'h' b'o' b'u' b'g' b'h' b' ' b'i' b't' b' ' b'h' b'a'\n",
      " b'd' b' ' b'b' b'e' b'e' b'n' b' ' b'm' b'a' b'n' b'y' b' ' b'y' b'e'\n",
      " b'a' b'r' b's' b' ' b's' b'i' b'n' b'c' b'e' b' ' b't' b'h' b'e' b' '\n",
      " b'R' b'i' b'd' b'd' b'l' b'e' b' ' b'f' b'a' b'm' b'i' b'l' b'y' b' '\n",
      " b'h' b'a' b'd'], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(2):\n",
    "    print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "color-flush",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T15:39:03.016340Z",
     "start_time": "2021-02-06T15:39:02.982784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'HARRY POTTER AND THE GOBLET OF FIRE\\n\\nCHAPTER ONE - THE RIDDLE HOUSE\\n\\n\\tThe villagers of Little Hangler'\n",
      "b'on still called it \"the Riddle House,\" even though it had been many years since the Riddle family had'\n",
      "b' lived there.  It stood on a hill overlooking the village, some of its windows boarded, tiles missing'\n",
      "b' from its roof, and ivy spreading unchecked over its face.  Once a fine-looking manor, and easily the'\n",
      "b' largest and grandest building for miles around, the Riddle House was now damp, derelict, and unoccup'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "    print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sunrise-princeton",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T15:39:03.024527Z",
     "start_time": "2021-02-06T15:39:03.018251Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "generic-deputy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T15:40:42.943393Z",
     "start_time": "2021-02-06T15:40:42.842507Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "viral-environment",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T15:40:50.725945Z",
     "start_time": "2021-02-06T15:40:50.697511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'HARRY POTTER AND THE GOBLET OF FIRE\\n\\nCHAPTER ONE - THE RIDDLE HOUSE\\n\\n\\tThe villagers of Little Hangle'\n",
      "Target: b'ARRY POTTER AND THE GOBLET OF FIRE\\n\\nCHAPTER ONE - THE RIDDLE HOUSE\\n\\n\\tThe villagers of Little Hangler'\n",
      "Input : b'on still called it \"the Riddle House,\" even though it had been many years since the Riddle family ha'\n",
      "Target: b'n still called it \"the Riddle House,\" even though it had been many years since the Riddle family had'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(2):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "french-laundry",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T15:40:50.737230Z",
     "start_time": "2021-02-06T15:40:50.728672Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ignored-dublin",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T15:40:50.750007Z",
     "start_time": "2021-02-06T15:40:50.739244Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "advance-substitute",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T15:40:50.762000Z",
     "start_time": "2021-02-06T15:40:50.751894Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True, \n",
    "                                   return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x=inputs\n",
    "        x=self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "        \n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "inside-carry",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T15:40:50.814105Z",
     "start_time": "2021-02-06T15:40:50.763569Z"
    }
   },
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size = len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim = embedding_dim,\n",
    "    rnn_units = rnn_units\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "spanish-richmond",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T15:40:54.113476Z",
     "start_time": "2021-02-06T15:40:50.818929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 82) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "boxed-burning",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T15:40:54.124905Z",
     "start_time": "2021-02-06T15:40:54.116279Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  20992     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    multiple                  3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  84050     \n",
      "=================================================================\n",
      "Total params: 4,043,346\n",
      "Trainable params: 4,043,346\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "senior-experiment",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T15:40:54.145696Z",
     "start_time": "2021-02-06T15:40:54.130419Z"
    }
   },
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "heard-kruger",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T15:40:54.215810Z",
     "start_time": "2021-02-06T15:40:54.150591Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "another-serum",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T15:40:54.239691Z",
     "start_time": "2021-02-06T15:40:54.218660Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpointsV2'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "broken-recipe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T15:40:54.250040Z",
     "start_time": "2021-02-06T15:40:54.242676Z"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-romance",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-06T15:40:50.725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "101/171 [================>.............] - ETA: 3:56 - loss: 3.5555"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-deviation",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-06T15:40:50.728Z"
    }
   },
   "outputs": [],
   "source": [
    "class NextStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature=temperature\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
    "        skip_ids = self.ids_from_chars(['','[UNK]'])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices = skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())]) \n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "    @tf.function\n",
    "    def generate_next_step(self, inputs, states=None):\n",
    "        # Convert strings to token IDs.\n",
    "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "        print('Input shape',input_ids.shape)\n",
    "        # Run the model.\n",
    "        # predicted_logits.shape is [batch, char, next_char_logits] \n",
    "        predicted_logits, states =  self.model(inputs=input_ids, states=states, \n",
    "                                          return_state=True)\n",
    "        # Only use the last prediction.\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits/self.temperature\n",
    "        # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
    "        predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "        # Sample the output logits to generate token IDs.\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "        # Convert from token ids to characters\n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "        \n",
    "        # Return the characters and model state.\n",
    "        return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-checklist",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-06T15:40:50.730Z"
    }
   },
   "outputs": [],
   "source": [
    "next_step_model = NextStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-anger",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-06T15:40:50.733Z"
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['Forest', 'the train', 'Platform', 'Bank', 'spell'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "    next_char, states = next_step_model.generate_next_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "\n",
    "print(result, '\\n\\n' + '_'*80)\n",
    "\n",
    "\n",
    "print(f\"\\nRun time: {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-exclusion",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-06T15:40:50.735Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.saved_model.save(next_step_model, 'potterv2')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
